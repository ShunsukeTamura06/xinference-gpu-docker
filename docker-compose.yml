version: '3.8'

services:
  xinference:
    build:
      context: .
      dockerfile: Dockerfile
      platforms:
        - linux/amd64
    container_name: xinference-gpu-server
    platform: linux/amd64
    restart: unless-stopped
    ports:
      - "9997:9997"
    volumes:
      - ./models:/root/.xinference/cache
      - ./logs:/app/logs
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HTTP_PROXY=http://hn02-outbound.gm.internal:8080
      - HTTPS_PROXY=http://hn02-outbound.gm.internal:8080
      - NO_PROXY=localhost,127.0.0.1,.internal,.local
    entrypoint: ["/bin/bash"]
    command: >
      -c "
        export http_proxy=http://hn02-outbound.gm.internal:8080 &&
        export https_proxy=http://hn02-outbound.gm.internal:8080 &&
        echo 'Acquire::http::Proxy \"http://hn02-outbound.gm.internal:8080\";' > /etc/apt/apt.conf.d/proxy.conf &&
        echo 'Acquire::https::Proxy \"http://hn02-outbound.gm.internal:8080\";' >> /etc/apt/apt.conf.d/proxy.conf &&
        echo 'Installing system packages...' &&
        apt-get update &&
        apt-get install -y python3 python3-pip python3-dev build-essential git wget curl &&
        echo 'Upgrading pip...' &&
        python3 -m pip install --upgrade pip &&
        echo 'Installing PyTorch with CUDA support...' &&
        pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 --extra-index-url https://download.pytorch.org/whl/cu118 &&
        echo 'Installing Xinference and dependencies...' &&
        pip install xinference[all] accelerate transformers sentence-transformers &&
        echo 'Starting Xinference server...' &&
        xinference-local --host=0.0.0.0 --port=9997
      "
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - xinference-network

networks:
  xinference-network:
    driver: bridge