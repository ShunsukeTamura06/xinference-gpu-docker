version: '3.8'

services:
  xinference:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: xinference-gpu-server
    restart: unless-stopped
    ports:
      - "9997:9997"
    volumes:
      - ./models:/root/.xinference/cache
      - ./logs:/app/logs
      # ホストのCUDA 12.9をマウント
      - /usr/local/cuda:/usr/local/cuda:ro
      - /usr/lib/aarch64-linux-gnu:/usr/lib/aarch64-linux-gnu:ro
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HTTP_PROXY=http://hn02-outbound.gm.internal:8080
      - HTTPS_PROXY=http://hn02-outbound.gm.internal:8080
      - NO_PROXY=localhost,127.0.0.1,.internal,.local
      # CUDA 12.9環境変数
      - PATH=/usr/local/cuda/bin:$PATH
      - LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/lib/aarch64-linux-gnu:$LD_LIBRARY_PATH
      - CUDA_HOME=/usr/local/cuda
    command: >
      bash -c "
        export http_proxy=http://hn02-outbound.gm.internal:8080 &&
        export https_proxy=http://hn02-outbound.gm.internal:8080 &&
        export PATH=/usr/local/cuda/bin:$PATH &&
        export LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/lib/aarch64-linux-gnu:$LD_LIBRARY_PATH &&
        export CUDA_HOME=/usr/local/cuda &&
        echo 'Acquire::http::Proxy \"http://hn02-outbound.gm.internal:8080\";' > /etc/apt/apt.conf.d/proxy.conf &&
        echo 'Acquire::https::Proxy \"http://hn02-outbound.gm.internal:8080\";' >> /etc/apt/apt.conf.d/proxy.conf &&
        echo 'Installing system packages for ARM64...' &&
        apt-get update &&
        apt-get install -y python3 python3-pip python3-dev build-essential git wget curl &&
        echo 'Checking CUDA installation...' &&
        ls -la /usr/local/cuda/ &&
        nvcc --version &&
        echo 'Upgrading pip...' &&
        python3 -m pip install --upgrade pip &&
        echo 'Installing PyTorch for ARM64 with CUDA 12.x support...' &&
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 &&
        echo 'Installing Xinference and dependencies...' &&
        pip install xinference[all] accelerate transformers sentence-transformers &&
        echo 'Testing CUDA availability...' &&
        python3 -c 'import torch; print(f\"CUDA available: {torch.cuda.is_available()}\"); print(f\"CUDA version: {torch.version.cuda}\"); print(f\"GPU count: {torch.cuda.device_count()}\")' &&
        echo 'Starting Xinference server...' &&
        xinference-local --host=0.0.0.0 --port=9997
      "
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - xinference-network

networks:
  xinference-network:
    driver: bridge