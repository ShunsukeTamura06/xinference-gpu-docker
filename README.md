# Xinference GPU Docker Setup for AWS T4G Instances

AWS T4G GPUæ­è¼‰ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆARM64ï¼‰ä¸Šã§Dockerã‚’ä½¿ç”¨ã—ã¦Xinferenceã‚µãƒ¼ãƒãƒ¼ã‚’GPUå¯¾å¿œã§å‹•ä½œã•ã›ã‚‹ãŸã‚ã®è¨­å®šã§ã™ã€‚ãƒ—ãƒ­ã‚­ã‚·ç’°å¢ƒã§ã®ä½¿ç”¨ã«ã‚‚å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚

## ğŸ“‹ å‰ææ¡ä»¶

- AWS T4G GPUæ­è¼‰ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆg5g.xlarge, g5g.2xlarge ãªã©ï¼‰
- Ubuntu 20.04+ ã¾ãŸã¯ Amazon Linux 2
- Docker ã¨ Docker Compose ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿
- NVIDIA GPU ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã¨CUDA 12.9ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿
- ä¼æ¥­ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç’°å¢ƒã®å ´åˆã¯ãƒ—ãƒ­ã‚­ã‚·è¨­å®š

## ğŸ”§ T4G GPU ã®ç‰¹å¾´

- **ARM64ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: ãƒã‚¤ãƒ†ã‚£ãƒ–ARM64ã‚³ãƒ³ãƒ†ãƒŠã‚’ä½¿ç”¨
- **CUDA 12.9å¯¾å¿œ**: ãƒ›ã‚¹ãƒˆã®CUDAã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’æ´»ç”¨
- **é«˜åŠ¹ç‡**: ARM64æœ€é©åŒ–ã•ã‚ŒãŸPyTorchã‚’ä½¿ç”¨

## ğŸŒ ãƒ—ãƒ­ã‚­ã‚·ç’°å¢ƒã§ã®è¨­å®š

ä¼æ¥­ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç’°å¢ƒã®å ´åˆã€ä»¥ä¸‹ã®ãƒ—ãƒ­ã‚­ã‚·è¨­å®šã‚’è¡Œã£ã¦ãã ã•ã„ï¼š

### 1. ç’°å¢ƒå¤‰æ•°ã®è¨­å®š

```bash
export HTTP_PROXY=http://hn02-outbound.gm.internal:8080
export HTTPS_PROXY=http://hn02-outbound.gm.internal:8080
export http_proxy=http://hn02-outbound.gm.internal:8080
export https_proxy=http://hn02-outbound.gm.internal:8080
export NO_PROXY=localhost,127.0.0.1,.internal,.local
export no_proxy=localhost,127.0.0.1,.internal,.local
```

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### 1. ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³

```bash
git clone https://github.com/ShunsukeTamura06/xinference-gpu-docker.git
cd xinference-gpu-docker
```

### 2. CUDAç’°å¢ƒç¢ºèª

```bash
# CUDA ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
nvcc --version

# GPUç¢ºèª
nvidia-smi

# CUDA ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ‘ã‚¹ç¢ºèª
ls -la /usr/local/cuda/
```

### 3. ãƒ—ãƒ­ã‚­ã‚·è¨­å®šï¼ˆä¼æ¥­ç’°å¢ƒã®å ´åˆï¼‰

```bash
# ãƒ—ãƒ­ã‚­ã‚·è¨­å®š
export HTTP_PROXY=http://hn02-outbound.gm.internal:8080
export HTTPS_PROXY=http://hn02-outbound.gm.internal:8080
```

### 4. ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œ

```bash
chmod +x setup.sh
./setup.sh
```

### 5. Xinferenceã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•

```bash
docker compose up -d
```

### 6. å‹•ä½œç¢ºèª

```bash
# ãƒ­ã‚°ã®ç¢ºèª
docker compose logs -f xinference

# GPUèªè­˜ç¢ºèª
curl http://localhost:9997/v1/models
```

## ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
xinference-gpu-docker/
â”œâ”€â”€ Dockerfile              # ARM64ç”¨Dockerãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ docker-compose.yml      # T4G GPUå¯¾å¿œè¨­å®š
â”œâ”€â”€ setup.sh               # ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ models/                # ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
â”œâ”€â”€ logs/                  # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
â””â”€â”€ README.md              # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
```

## ğŸ”§ è¨­å®šã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

### ãƒ—ãƒ­ã‚­ã‚·è¨­å®šã®å¤‰æ›´

`docker-compose.yml`ã§ãƒ—ãƒ­ã‚­ã‚·URLã‚’å¤‰æ›´ï¼š

```yaml
environment:
  - HTTP_PROXY=http://your-proxy:port
  - HTTPS_PROXY=http://your-proxy:port
```

### CUDA ãƒ‘ã‚¹ã®å¤‰æ›´

ãƒ›ã‚¹ãƒˆã®CUDAã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ‘ã‚¹ãŒç•°ãªã‚‹å ´åˆï¼š

```yaml
volumes:
  - /your/cuda/path:/usr/local/cuda:ro
```

### ãƒ¡ãƒ¢ãƒªåˆ¶é™ã®è¨­å®š

```yaml
deploy:
  resources:
    limits:
      memory: 8G
```

## ğŸ“Š ä½¿ç”¨æ–¹æ³•

### Xinference Web UI ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹

ãƒ–ãƒ©ã‚¦ã‚¶ã§ `http://YOUR_SERVER_IP:9997` ã«ã‚¢ã‚¯ã‚»ã‚¹

### APIçµŒç”±ã§ã®åˆ©ç”¨

```bash
# åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®ç¢ºèª
curl http://localhost:9997/v1/models

# ãƒ¢ãƒ‡ãƒ«ã®èµ·å‹•ä¾‹
curl -X POST http://localhost:9997/v1/models \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "vicuna-v1.3",
    "model_size_in_billions": 7
  }'
```

### Python ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§ã®åˆ©ç”¨

```python
from xinference.client import Client

client = Client("http://localhost:9997")

# ãƒ¢ãƒ‡ãƒ«ã®èµ·å‹•
model = client.launch_model(
    model_name="vicuna-v1.3",
    model_size_in_billions=7
)

# æ¨è«–ã®å®Ÿè¡Œ
response = model.chat(
    "ã“ã‚“ã«ã¡ã¯ã€å…ƒæ°—ã§ã™ã‹ï¼Ÿ"
)
print(response)
```

## ğŸ” ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### T4G GPUç‰¹æœ‰ã®å•é¡Œ

#### CUDAèªè­˜ã•ã‚Œãªã„å ´åˆ

```bash
# ãƒ›ã‚¹ãƒˆã®CUDAç¢ºèª
nvcc --version
ls -la /usr/local/cuda/

# ã‚³ãƒ³ãƒ†ãƒŠå†…ã§CUDAç¢ºèª
docker compose exec xinference nvcc --version
docker compose exec xinference python3 -c "import torch; print(torch.cuda.is_available())"
```

#### ARM64ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼

```bash
# ARM64ç”¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒæ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
docker compose exec xinference uname -a
docker compose exec xinference python3 -c "import torch; print(torch.__version__)"
```

### ä¸€èˆ¬çš„ãªå•é¡Œ

#### ãƒ—ãƒ­ã‚­ã‚·é–¢é€£ã‚¨ãƒ©ãƒ¼

```bash
# ãƒ—ãƒ­ã‚­ã‚·è¨­å®šç¢ºèª
echo $HTTP_PROXY

# æ‰‹å‹•ã§ãƒ—ãƒ­ã‚­ã‚·è¨­å®š
export HTTP_PROXY=http://hn02-outbound.gm.internal:8080
export HTTPS_PROXY=http://hn02-outbound.gm.internal:8080

# å†ãƒ“ãƒ«ãƒ‰
docker compose build --no-cache
```

#### ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼

```bash
# ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
docker stats xinference-gpu-server
```

## ğŸ“ˆ T4G GPUæœ€é©åŒ–

### 1. ARM64æœ€é©åŒ–ã•ã‚ŒãŸãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ç”¨

```yaml
volumes:
  - /usr/lib/aarch64-linux-gnu:/usr/lib/aarch64-linux-gnu:ro
```

### 2. CUDA 12.9æ´»ç”¨

```yaml
environment:
  - CUDA_HOME=/usr/local/cuda
  - LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
```

### 3. T4G GPUæ¨å¥¨è¨­å®š

```yaml
environment:
  - CUDA_VISIBLE_DEVICES=0
  - NVIDIA_VISIBLE_DEVICES=all
```

## ğŸ› ï¸ ã‚³ãƒãƒ³ãƒ‰ä¸€è¦§

| ã‚³ãƒãƒ³ãƒ‰ | èª¬æ˜ |
|---------|------|
| `docker compose up -d` | ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰ |
| `docker compose down` | ã‚µãƒ¼ãƒãƒ¼åœæ­¢ |
| `docker compose logs -f` | ãƒ­ã‚°ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤º |
| `docker compose ps` | ã‚³ãƒ³ãƒ†ãƒŠçŠ¶æ…‹ç¢ºèª |
| `docker compose restart` | ã‚µãƒ¼ãƒãƒ¼å†èµ·å‹• |
| `docker compose build --no-cache` | å¼·åˆ¶å†ãƒ“ãƒ«ãƒ‰ |

## ğŸ†˜ ã‚µãƒãƒ¼ãƒˆ

å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆï¼š

1. [Issues](https://github.com/ShunsukeTamura06/xinference-gpu-docker/issues) ã§æ—¢å­˜ã®å•é¡Œã‚’ç¢ºèª
2. æ–°ã—ã„Issueã‚’ä½œæˆï¼ˆãƒ­ã‚°ã¨ç’°å¢ƒæƒ…å ±ã‚’å«ã‚ã‚‹ï¼‰
3. [Xinferenceå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://inference.readthedocs.io/) ã‚’å‚ç…§

## ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

MIT License

## ğŸ¤ ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³

ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚„Issueã¯æ­“è¿ã—ã¾ã™ï¼